{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_transactions(path='./transactions_train.csv', parquet_path='transactions.pqt'):\n",
    "    transaction_columns = [\n",
    "        't_dat',\n",
    "        'customer_id',\n",
    "        'article_id'\n",
    "    ]\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"Loading cached Parquet file: {parquet_path}\")\n",
    "        return cudf.read_parquet(parquet_path)\n",
    "    \n",
    "    \n",
    "    print(f\"Processing raw CSV file: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[transaction_columns]\n",
    "    \n",
    "    gdf = cudf.DataFrame.from_pandas(df)\n",
    "    gdf['customer_id'] = gdf['customer_id'].str[-16:].str.hex_to_int().astype('int64')\n",
    "    gdf['article_id'] = gdf['article_id'].astype('int32')\n",
    "    gdf['t_dat'] = cudf.to_datetime(gdf['t_dat'])\n",
    "    \n",
    "    gdf.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Saved processed data to: {parquet_path}\")\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached Parquet file: transactions.pqt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = load_and_preprocess_transactions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = data.loc[(data[\"t_dat\"] >= datetime(2020, 9, 8)) & (data[\"t_dat\"] < datetime(2020, 9, 16))]\n",
    "train2 = data.loc[(data[\"t_dat\"] >= datetime(2020, 9, 1)) & (data[\"t_dat\"] < datetime(2020, 9, 8))]\n",
    "train3 = data.loc[(data[\"t_dat\"] >= datetime(2020, 8, 23)) & (data[\"t_dat\"] < datetime(2020, 9, 1))]\n",
    "train4 = data.loc[(data[\"t_dat\"] >= datetime(2020, 8, 15)) & (data[\"t_dat\"] < datetime(2020, 8, 23))]\n",
    "\n",
    "val = data.loc[data[\"t_dat\"] >= datetime(2020, 9, 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_articles(df):\n",
    "    return df.groupby('customer_id').agg({'article_id': 'collect'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "positive_items_per_user1 = group_articles(train1)\n",
    "positive_items_per_user2 = group_articles(train2)\n",
    "positive_items_per_user3 = group_articles(train3)\n",
    "positive_items_per_user4 = group_articles(train4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = cudf.concat([train1, train2, train3, train4], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date = datetime(2020, 9, 16)\n",
    "train['days_diff'] = (ref_date - train['t_dat']).dt.days\n",
    "train['pop_factor'] = 1 / (train['days_diff'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_items_group = train.groupby(['article_id'])['pop_factor'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_total_count = train.groupby(['article_id'])['article_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_total_count = train.groupby(['customer_id'])['customer_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['feedback'] = 1\n",
    "train= train.groupby(['customer_id', 'article_id']).agg({'feedback': 'sum'}).reset_index()\n",
    "train = train.merge(popular_items_group, on='article_id')\n",
    "train['feedback'] = train['feedback'] / train['pop_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['feedback'] = train['feedback'].clip(upper=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.044811e+06\n",
       "mean     8.252942e-01\n",
       "std      1.455205e+00\n",
       "min      5.373625e-03\n",
       "25%      6.460413e-02\n",
       "50%      1.771086e-01\n",
       "75%      6.486035e-01\n",
       "max      5.000000e+00\n",
       "Name: feedback, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train['feedback'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_pop = data[(data['t_dat'] >= datetime(2020, 9, 1)) & (data['t_dat'] < datetime(2020, 9, 16))]\n",
    "train_pop['pop_factor'] = 1 / (ref_date - train_pop['t_dat']).dt.days\n",
    "popular_items_group_cu = train_pop.groupby('article_id')['pop_factor'].sum()\n",
    "\n",
    "values = popular_items_group.to_pandas().values\n",
    "keys = popular_items_group.index.to_pandas().values\n",
    "_, popular_items = zip(*sorted(zip(values, keys))[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_most_freq_next_item(user_group):\n",
    "    next_items = {}\n",
    "    for user in tqdm(user_group.keys()):\n",
    "        items = user_group[user]\n",
    "        for i,item in enumerate(items[:-1]):\n",
    "            if item not in next_items:\n",
    "                next_items[item] = []\n",
    "            if item != items[i+1]:\n",
    "                next_items[item].append(items[i+1])\n",
    "\n",
    "    pred_next = {}\n",
    "    for item in next_items:\n",
    "        if len(next_items[item]) >= 5:\n",
    "            most_common = Counter(next_items[item]).most_common()\n",
    "            ratio = most_common[0][1]/len(next_items[item])\n",
    "            if ratio >= 0.1:\n",
    "                pred_next[item] = most_common[0][0]\n",
    "            \n",
    "    return pred_next\n",
    "\n",
    "user_group = train.groupby(['customer_id'])['article_id'].apply(list)\n",
    "pred_next = get_most_freq_next_item(user_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD Training ðŸ”¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from reco.recommender import FunkSVD\n",
    "from reco.metrics import rmse\n",
    "\n",
    "# k = number of dimensions of the latent embedding. formatizer dict takes in names of the columns\n",
    "# for user, item and values/feedback/ratings respectively.\n",
    "\n",
    "svd = FunkSVD(k=8, learning_rate=0.008, regularizer = .01, iterations = 80, method = 'stochastic', bias=True)\n",
    "svd.fit(X=train, formatizer={'user':'customer_id', 'item':'article_id', 'value':'feedback'},verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation ðŸ”¹\n",
    "\n",
    "This is mostly based on [this notebook](https://www.kaggle.com/mayukh18/time-decaying-popularity-benchmark-0-0216) where I have used same pipeline. Will use SVD for re-ranking. Will read all the data anew and train a new model on our new train set with new date ranges for submission. This will align us with the aforementioned notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=12):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "positive_items_val = val.groupby(['customer_id'])['article_id'].apply(list)\n",
    "val_users = positive_items_val.keys()\n",
    "val_items = []\n",
    "\n",
    "for i,user in tqdm(enumerate(val_users)):\n",
    "    val_items.append(positive_items_val[user])\n",
    "    \n",
    "print(\"Total users in validation:\", len(val_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal way of prediction without the SVD reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "outputs = []\n",
    "cnt = 0\n",
    "\n",
    "popular_items = list(popular_items)\n",
    "\n",
    "for user in tqdm(val_users):\n",
    "    user_output = []\n",
    "    if user in positive_items_per_user1.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n",
    "        user_output += list(most_common_items_of_user.keys())[:12]\n",
    "    if user in positive_items_per_user2.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n",
    "        user_output += list(most_common_items_of_user.keys())[:12]\n",
    "    if user in positive_items_per_user3.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n",
    "        user_output += list(most_common_items_of_user.keys())[:12]\n",
    "    if user in positive_items_per_user4.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n",
    "        user_output += list(most_common_items_of_user.keys())[:12]\n",
    "    \n",
    "    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n",
    "    \n",
    "    user_output += list(popular_items[:12 - len(user_output)])\n",
    "    outputs.append(user_output)\n",
    "    \n",
    "print(\"mAP Score on Validation set:\", mapk(val_items, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, prediction WITH the SVD reranking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "outputs = []\n",
    "cnt = 0\n",
    "\n",
    "popular_items = list(popular_items)\n",
    "userindexes = {svd.users[i]:i for i in range(len(svd.users))}\n",
    "\n",
    "for user in tqdm(val_users):\n",
    "    user_output = []\n",
    "    if user in positive_items_per_user1.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = svd.items.index(k)\n",
    "                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user2.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = svd.items.index(k)\n",
    "                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user3.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = svd.items.index(k)\n",
    "                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user4.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = svd.items.index(k)\n",
    "                pred_value = np.dot(svd.userfeatures[user_index], svd.itemfeatures[itemindex].T) + svd.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n",
    "    \n",
    "    user_output += list(popular_items[:12 - len(user_output)])\n",
    "    outputs.append(user_output)\n",
    "    \n",
    "print(\"mAP Score on Validation set:\", mapk(val_items, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent jump of 0.001 I would say with 4 weeks of training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "We will do all the same things all over again just with different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train1 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,16)) & (data['t_dat'] < datetime.datetime(2020,9,23))]\n",
    "train2 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,9,8)) & (data['t_dat'] < datetime.datetime(2020,9,16))]\n",
    "train3 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,31)) & (data['t_dat'] < datetime.datetime(2020,9,8))]\n",
    "train4 = data.loc[(data[\"t_dat\"] >= datetime.datetime(2020,8,23)) & (data['t_dat'] < datetime.datetime(2020,8,31))]\n",
    "\n",
    "positive_items_per_user1 = train1.groupby(['customer_id'])['article_id'].apply(list)\n",
    "positive_items_per_user2 = train2.groupby(['customer_id'])['article_id'].apply(list)\n",
    "positive_items_per_user3 = train3.groupby(['customer_id'])['article_id'].apply(list)\n",
    "positive_items_per_user4 = train4.groupby(['customer_id'])['article_id'].apply(list)\n",
    "\n",
    "train = pd.concat([train1, train2], axis=0)\n",
    "train['pop_factor'] = train['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\n",
    "popular_items_group = train.groupby(['article_id'])['pop_factor'].sum()\n",
    "\n",
    "_, popular_items = zip(*sorted(zip(popular_items_group, popular_items_group.keys()))[::-1])\n",
    "\n",
    "user_group = pd.concat([train1, train2, train3, train4], axis=0).groupby(['customer_id'])['article_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "train = pd.concat([train1, train2, train3, train4], axis=0)\n",
    "train['pop_factor'] = train['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days**2)\n",
    "popular_items_group = train.groupby(['article_id'])['pop_factor'].sum()\n",
    "\n",
    "train['feedback'] = 1\n",
    "train = train.groupby(['customer_id', 'article_id']).sum().reset_index()\n",
    "\n",
    "train['feedback'] = train.apply(lambda row: row['feedback']/popular_items_group[row['article_id']], axis=1)\n",
    "\n",
    "train['feedback'] = train['feedback'].apply(lambda x: 5.0 if x>5.0 else x)\n",
    "train.drop(['price', 'sales_channel_id'], axis=1, inplace=True)\n",
    "train['feedback'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from reco.recommender import FunkSVD\n",
    "from reco.metrics import rmse\n",
    "\n",
    "f = FunkSVD(k=8, learning_rate=0.005, regularizer = .01, iterations = 200, method = 'stochastic', bias=True)\n",
    "f.fit(X=train, formatizer={'user':'customer_id', 'item':'article_id', 'value':'feedback'},verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "cnt = 0\n",
    "\n",
    "popular_items = list(popular_items)\n",
    "userindexes = {f.users[i]:i for i in range(len(f.users))}\n",
    "\n",
    "for user in tqdm(submission['customer_id']):\n",
    "    user_output = []\n",
    "    if user in positive_items_per_user1.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user1[user]).most_common()}\n",
    "        \n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = f.items.index(k)\n",
    "                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user2.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user2[user]).most_common()}\n",
    "        \n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = f.items.index(k)\n",
    "                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user3.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user3[user]).most_common()}\n",
    "        \n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = f.items.index(k)\n",
    "                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    if user in positive_items_per_user4.keys():\n",
    "        most_common_items_of_user = {k:v for k, v in Counter(positive_items_per_user4[user]).most_common()}\n",
    "        \n",
    "        user_index = userindexes[user]\n",
    "        new_order = {}\n",
    "        for k in list(most_common_items_of_user.keys())[:20]:\n",
    "            try:\n",
    "                itemindex = f.items.index(k)\n",
    "                pred_value = np.dot(f.userfeatures[user_index], f.itemfeatures[itemindex].T) + f.item_bias[0, itemindex]\n",
    "            except:\n",
    "                pred_value = most_common_items_of_user[k]\n",
    "            new_order[k] = pred_value\n",
    "        user_output += [k for k, v in sorted(new_order.items(), key=lambda item: item[1])][:12]\n",
    "        \n",
    "    user_output += [pred_next[item] for item in user_output if item in pred_next and pred_next[item] not in user_output]      \n",
    "    \n",
    "    user_output += list(popular_items[:12 - len(user_output)])\n",
    "    outputs.append(user_output)\n",
    "    \n",
    "str_outputs = []\n",
    "for output in outputs:\n",
    "    str_outputs.append(\" \".join([str(x) for x in output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission['prediction'] = str_outputs\n",
    "submission.to_csv(\"submissions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That's it! Upvote and Enjoy!\n",
    "Examples of FunkSVD and FM are over at https://github.com/mayukh18/reco/tree/master/examples. I'll try to add an example of [Wide And Deep Network](https://arxiv.org/pdf/1606.07792.pdf) in the coming days. That is also a pretty cool model to try next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 3103714,
     "sourceId": 31254,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30162,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
